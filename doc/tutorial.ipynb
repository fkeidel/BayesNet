{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tutorial\n",
    "This is a short tutorial to get started with Bayesian Networks and the BayesNet library.\n",
    "\n",
    "## Random variables and probability distributions\n",
    "\n",
    "The real world is uncertain ... weather, stock prices, sensor readings etc. A mathematically sound way to deal with uncertainty is to use **probabilities**, e.g. the probability that it will rain on a random day:\n",
    "$$ P(Rain=true)=0.16 $$\n",
    "\"Rain\" is a **discrete random variable** that can have values $\\{false, true\\}$\n",
    "\n",
    "Probabilities add to 1 so:\n",
    "$$ P(Rain=false) = 1 - P(Rain=true) = 0.84 $$\n",
    "There is a shortcut to write both probabilities, for \"Rain=true\" and \"Rain=false\" at once:\n",
    "$$ p(Rain) = \\begin{pmatrix} p(Rain=false) \\\\ p(Rain=true) \\end{pmatrix} = \\begin{pmatrix} 0.84 \\\\ 0.16 \\end{pmatrix} $$\n",
    "\n",
    "$P(Rain)$ is a **probability distribution** over all possible values of Rain. It can also be represented as a table:\n",
    "\n",
    "![](pd_rain.svg)\n",
    "\n",
    "Notice that the values sum to one, since the values represent a probability distribution.\n",
    "\n",
    "In **graphical models**, like **Bayesian networks**, random variables are represented as cirles:\n",
    "\n",
    "![](one_variable.svg)\n",
    "\n",
    "Things get interesting, when you have more than one variable in your model. Then, it can happen, that **one variable influences another variable**.\n",
    "The graphical way to represent this relation between the two variables is an arrow, e.g. the season influences the probability for rain:\n",
    "\n",
    "![](season_rain.svg)\n",
    "\n",
    "This could be read as:\n",
    "* season **influences** or **causes** rain or\n",
    "* rain **depends on** or **is influenced by** season\n",
    "\n",
    "Assume that \"Season\" has two values $ \\{winter, summer\\} $, then the probability that it rains is higher, when we know it is winter, than when it is summer.  \n",
    "This is now a **conditional probability**: \n",
    "$$P(Rain|Season)$$\n",
    "\n",
    "The bar \"|\" is called the \"conditioning bar\" and reads as \"given\": the probabilty of \"Rain\" given the \"Season\". Knowing the season, would could change our belief, whether it would rain today.\n",
    "\n",
    "We can express this as a table:\n",
    "\n",
    "![](cpd_rain_given_season.svg)\n",
    "\n",
    "This is a **conditional probability table (CPD)**. The first line is $P(Rain|Season=winter)$ and the second line is $P(Rain|Season=summer)$\n",
    "\n",
    "Note that the values in the rows of the table sum to one. Each row represents a (conditional) probability distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Chain Rule for Bayesian Networks\n",
    "\n",
    "Now, the **key to understand Bayesian Networks** is, that the diagram above with the two random variables \"Season\" and \"Rain\" represents a joint probability. It is the joint probability of season and rain: $ P(Season,Rain)$.\n",
    "\n",
    "And the rules for Bayesian Networks say that if you want to calculate the joint probability corresponding to a diagram with random variables, then you have one factor per variable, where a factor for a variable without incoming edges is a probability, e.g. $P(Season)$ and a factor with incoming edges is a conditional probability, e.g. $P(Rain|Season)$, and you multiply all these factors.\n",
    "\n",
    "So in the case above, $ P(Season, Rain)$ would be:\n",
    "\n",
    "$$ P(Season,Rain) = P(Season) * P(Rain|Season) $$\n",
    "\n",
    "Variables that we condition on, like \"Season\", are called \"parents\" (pa). \"Season\" is a parent of \"Rain\". The general formula to calculate the joint probability of a Bayesian network is:\n",
    "\n",
    "$$ P(X_1, X_2, ... , X_n) = \\prod_i P(X_i|pa(X_i)) $$\n",
    "\n",
    "This is called the **Chain Rule** for Bayesian Networks.\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> A Bayesian network is just a graphical representation of a joint probability distribution. The arrows in the diagram indicate, how this joint distribution factorizes into (conditional) probabilities</div>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Inference in Bayesian Networks\n",
    "\n",
    "You might say: **\"What is the benefit of knowing the joint distribution and how it factorizes?\"**\n",
    "\n",
    "The benefit of knowing the joint distribution and its factorization is, that we can derive (infer) values from it, that are of interest and are not given.\n",
    "\n",
    "For example, from the above joint and factorization of \"Season\" and \"Rain\" $P(Season,Rain) = P(Season) * P(Rain|Season)$ we could compute the following distributions:\n",
    "* $P(Season|Rain)$: the probability of the season, given it is raining\n",
    "* $P(Rain)$: the (unconditional/marginal) probability of rain\n",
    "\n",
    "Calculating such values is called **inference**.\n",
    "\n",
    "You can think of a joint distribution as a database, that you can query for information. The information in this \"database\" is linked like in a brain. And when you query the database, these links are considered when calculating the result.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Factors\n",
    "\n",
    "When doing calculations based on Bayesian Networks, we often do not calculate with probabilities, but with \"factors\". **Factors** are similar to probability distributions, but more general:\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-warning\"><b>Factor:</b> A factor is simply a function that maps a certain configuration (aka. assignments) of its arguments to a real value</div>\n",
    "\n",
    "$$ f(arg_1,arg_2, ... , arg_n) \\mapsto \\mathbb{R} $$\n",
    "\n",
    "If the arguments are discrete values, then we can represent a factor, similar to a (conditional) probability, as a table.\n",
    "But in contrast to conditional probability tables (CPDs), where the values are stored in a table/matrix, the values in factors are contained in a \"flat\" vector. Furthermore, their values do not have to sum to 1 and the values can even be negative.\n",
    "\n",
    "We can convert probability distributions in factors. In the diagram you see the probability distributions of \"Season\" and \"Rain\" on the left and their corresponding factors on the right.\n",
    "\n",
    "![](cpds_and_factors.svg)\n",
    "\n",
    "Note: In the BayesNet library, we follow the convention that the value of the variable in the first column changes most frequently.\n",
    "\n",
    "Here is how you would define a probability, like $P(Rain)$ and a CPD, like $P(Rain|Season)$ in the BayesNet library:\n",
    "\n",
    "```\n",
    "Factor rain{ {0}, {2}, { 0.86, 0.14 } };\n",
    "\n",
    "Factor rain_given_season{ {1,0}, {2,2}, {0.82,0.18,0.86,0.14} };\n",
    "```\n",
    "\n",
    "A factor has 3 attributes:\n",
    "\n",
    "1. var:  list of variables (arguments) of the factor, aka. scope\n",
    "2. card: list of the cardinalities of the variables in var\n",
    "3. val:  list of values\n",
    "\n",
    "Variables in BayesNet are identified with an unique id. The ids start with 0 and are contiguous. You can make the code more readable, if you define an enum for the variables:\n",
    "\n",
    "```\n",
    "enum VarIds : uint32_t {\n",
    "\tSEASON,\n",
    "\tRAIN\n",
    "};\n",
    "\n",
    "Factor rain{ {RAIN}, {2},   { 0.86, 0.14 };\n",
    "\n",
    "Factor rain_given_season{ {RAIN,SEASON}, {2,2}, {0.82,0.18,0.86,0.14} };\n",
    "```\n",
    "\n",
    "The set of variables var $X_1 ... X_n$ is called the **scope** of the factor. That is the set of arguments, that a factor takes. A concrete configuration of the arguments is called **assignment**. By providing an assignment, you can lookup the corresponding value in the vector of values.\n",
    "\n",
    "For example, in case of the factor rain_given_season\n",
    "\n",
    "![](factor_rain_season.svg)\n",
    "\n",
    "with\n",
    "\n",
    "```\n",
    "const auto value = rain_given_season({0,1});\n",
    "```\n",
    "\n",
    "you would get the value 0.86 from the 3rd line in the vector.\n",
    "\n",
    "Again, you can make the code more readable using enumerations for the arguments of the factors. Here, all arguments are binary values:\n",
    "\n",
    "```\n",
    "enum BinaryValue : uint32_t {\n",
    "\tFALSE,\n",
    "\tTRUE\n",
    "};\n",
    "\n",
    "const auto value = rain_given_season({FALSE,TRUE});\n",
    "```\n",
    "\n",
    "Note: When working with factors, there is no notion of direction. From f(Season,Rain) we do not know that \"Season\" has an influence on \"Rain\". So without knowing the original graph with the directed edges, we cannot convert a factor back to a conditional probability distribution.\n",
    "\n",
    "Why would we use factors instead of probabilites for calculating values, when we loose information using factors? The reason is, that factor calculation can be used to calculate values also in other kind of graphical models like [Markov Random Fields](https://en.wikipedia.org/wiki/Markov_random_field) (undirected graphical model) and [Influence diagrams](https://en.wikipedia.org/wiki/Influence_diagram) (decision problems). \n",
    "\n",
    "![](MRF_ID.svg)\n",
    "\n",
    "On the left side is a Markov Random field with undirected edges. On the right side is an Influence Diagram with special nodes for decision factors (rectangles) and utility functions (diamonds).\n",
    "\n",
    "So \"factor calculation\" is an unifiying approach. Furthermore, often, we are only interested in calculating marginals, like $P(Rain)$, so there is no need to create a CPD from a factor. Lastly, when you know the original Bayes Net with the arrows, you can convert the factors to CPDs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Factor operations\n",
    "\n",
    "After instantiating the factors, you can call operations defined for factors, like FactorProduct, FactorSum, Marginalize, ObserveEvidence (factor reduction) or Normalize. These are the building blocks for algorithms like VariableElimination or JunctionTree.\n",
    "\n",
    "## FactorProduct\n",
    "The FactorProduct multiplies two factors. If the factors represent probabilites or conditional probabilites, then the product will be the joint distribution.\n",
    "\n",
    "Example:\n",
    "\n",
    "![](factor_product.svg)\n",
    "\n",
    "The FactorProduct is defined as a pointwise multiplication. For instance, for the two factors in the diagram above, the first row of the first factor has assignment Season=0 and value=0.5. The value 0.5 is then multiplied with all values of the second factor, that have Season=0, which are the first two rows.\n",
    "\n",
    "The general rule is, that you take a row of the first factor and you multiply the value of this row with the values of the rows of the second factor, for which the assignement of the first factor matches the assignment of the second factor.\n",
    "\n",
    "In BayesNet, the variables in the result of a factor operation are ordered. So in the resulting factor for the joint distribution (most right factor in the above diagram), Season with id 0 is in the first column and Rain with id 1 is in the second column. Notice that reordering the variables in a factor changes also the order of the values in the factor. In the example, reordering of the variables of the result swaps row 2 and 3.\n",
    "\n",
    "```\n",
    "Factor season{ {SEASON}, {2}, { 0.5, 0.5 } };\n",
    "Factor rain_given_season{ {RAIN,SEASON}, {2,2}, {0.82,0.18,0.86,0.14} };\n",
    "Factor joint = FactorProduct(season, rain_given_season);\n",
    "```\n",
    "Result:\n",
    "```\n",
    "joint:\n",
    "var=[0,1]\n",
    "card=[2,2]\n",
    "val=[0.41,0.43,0.09,0.07]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## FactorSum\n",
    "FactorSum is like FactorProduct, except that corresponding values in the two factors are summed instead of multiplied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Marginalization\n",
    "Marginalization removes a variable or multiple variables from the scope of a factor:\n",
    "$$ f(Season, Rain) \\to marginalize\\ Season \\to f(Rain) $$\n",
    "\n",
    "![](factor_marginalization.svg)\n",
    "\n",
    "Marginalization is done by removing the column(s) of the variable(s) to be removed and adding all values with the same remaining assignment. When we remove the column Season, there are two entries for Rain=0 and two entries for Rain=1 that get added.\n",
    "\n",
    "```\n",
    "Factor marginal = joint.Marginalize({SEASON});\n",
    "```\n",
    "Result:\n",
    "```\n",
    "marginal:\n",
    "var=[1]\n",
    "card=[2]\n",
    "val=[0.84,0.16]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Observe evidence\n",
    "\n",
    "If the values of some variables are observed, we can incorporate this knowledge into a factor or a list of factors.\n",
    "Incorporating evidence in a factor is done by setting all values in the factor to zero, that are not consistent with the evidence.\n",
    "In the example, we observe (i.e. know for sure), that it is summer (SEASON=TRUE), so all values in the factor that have SEASON=FALSE are set to 0.\n",
    "\n",
    "![](observe_evidence.svg)\n",
    "\n",
    "\"Evidence\" in BayeseNet is a list of variable/value pairs for the observed variables.\n",
    "```\n",
    "const Evidence evidence{ {SEASON, TRUE} };\n",
    "rain_given_season.ObserveEvidence(evidence);\n",
    "```\n",
    "gives\n",
    "```\n",
    "rain_given_season(Season=true):\n",
    "var=[1,0]\n",
    "card=[2,2]\n",
    "val=[0,0,0.86,0.14]\n",
    "```\n",
    "After incorporating the evidence, you can marginalize out the observed variables, because they are constants that will not change.\n",
    "ObserveEvidence has also a boolean argument \"marginalize\", with which you can indicate, that the observed variables should be removed from the scope of the factor.\n",
    "```\n",
    "bool marginalize{ true };\n",
    "rain_given_season.ObserveEvidence(evidence, marginalize);\n",
    "```\n",
    "This gives:\n",
    "```\n",
    "rain_given_season(Season=true):\n",
    "var=[1]\n",
    "card=[2]\n",
    "val=[0.86,0.14]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Normalize\n",
    "Normalize() normalizes the values of a factor to sum to 1. This is useful, if the result of a factor calculation is a marginal probability and you want that the values sum to 1.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Inference algorithms\n",
    "\n",
    "Now you know how to calculate values of interest in Bayesian Networks with factor operations like FactorProduct, Marginalize and ObserveEvidence.\n",
    "\n",
    "Up to now, we had only small networks, with only one or two variables. At the beginning, we started with a single random variable Rain, specifying the probability distribution for rain on a random day. Then we refined the model by adding a dependency to the season, because we stated that it rains more in winter than in summer. We can continue to refine the model more and more. Let's add a dependency to the country. In some countries it rains more than in others. We can also create a dependency between Season and Country, because in some countries, like Finland, the summer is short and the winter long, whereas in Egypt, it is vice versa.\n",
    "\n",
    "![](country_season_rain.svg)\n",
    "\n",
    "The code for the right diagram would be:\n",
    "\n",
    "```\n",
    "   // let's add another variable country\n",
    "   // add additional id \n",
    "   const uint32_t COUNTRY{ 2 };\n",
    "   // country has two values {false, true} which mean {Finland, Egypt}\n",
    "   // the numbers in the factor represent the number of inhabitants in millions\n",
    "   // if the factor should represent a probability, we have to normalize the factor\n",
    "   Factor country{ {COUNTRY}, {2}, { 5, 100 } };\n",
    "   country.Normalize();\n",
    "   // now change the factors for Season and Rain to have an additional dependency to country\n",
    "   Factor season_given_country{ {SEASON,COUNTRY}, {2,2}, {0.8,0.2,0.2,0.8} };\n",
    "   Factor rain_given_country_and_season{ {RAIN,SEASON,COUNTRY}, {2,2,2}, {0.82,0.18,0.86,0.14,0.4,0.6,0.6,0.4} };\n",
    "\n",
    "```\n",
    "\n",
    "So refining the model adds random variables and dependencies. It makes the model more accurate, but also more complex. Manual calculations with factor operations soon get tedious.\n",
    "\n",
    "What we need are general algorithm, that can be applied to any Bayesian Network, without having to manually specify the factor operations to get a value of interest."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Variable Elimination\n",
    "Applying manual factor operations on Bayesian Networks is tedious, but can also be very inefficient. \n",
    "If you have a network with many variables, you could first calculate the joint distribution (which contains the knowledge about the whole system) and then marginalize out all variables execpt the variables you are interested in. If you would take a closer look at the operations done in the marginalization step, you would see that some terms would get calculated more than once.\n",
    "\n",
    "VariableElimination is an algorithm for Bayesian Networks that uses Dynamic Programming to store intermediate terms until they are needed again, to avoid duplicated calculations.\n",
    "\n",
    "Let's see how it works. VariableElimination takes in a list of factors (i.e. the Bayesian Network) and a list of variables to eliminate and returns a list of remaining factors after the elimination of the given variables. \n",
    "\n",
    "If we eliminate Country and Season, we get the marginal probability of Rain.\n",
    "\n",
    "```\n",
    "   std::vector <Factor> factors{\n",
    "      country,\n",
    "      season_given_country,\n",
    "      rain_given_country_and_season\n",
    "   };\n",
    "\n",
    "   auto f(factors);  // make copy to preserve original factors\n",
    "   // eliminate Season and Country gives the marginal of Rain\n",
    "   VariableElimination(f, { SEASON,COUNTRY });\n",
    "   const auto m_rain = f.front();\n",
    "```\n",
    "If you want the probabilities of other variables, you have to run Variable Elimination again.\n",
    "For example, to calculate the marginal probability of Season:\n",
    "```\n",
    "   f = factors;  // restore original factors\n",
    "   // eliminate Rain and Country gives the marginal of Season\n",
    "   VariableElimination(f, { RAIN,COUNTRY });\n",
    "   const auto m_season = f.front();\n",
    "   std::cout << \"m_season:\\n\" << m_season;\n",
    "```\n",
    "If you want to calculate multiple values from a Bayes Net, use the more efficient Clique Tree algorithm, to avoid duplicate calculations.\n",
    "\n",
    "For another example, see [Water Sprinkler](../examples/water_sprinkler/water_sprinkler_variable_elimination.cpp)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clique Tree (Junction Tree)\n",
    "\n",
    "VariableElimination is an efficient algorithm for networks that are trees. In a tree, you have only one path between two variables. If you have a network that has variables with more than one path between, you can cluster variables into common factors, called cliques. These cliques then form again a tree, where we can efficiently do the inference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}